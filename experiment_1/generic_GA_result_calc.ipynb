{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
    "from generic_optimizer import GenericOptimizer\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "import re\n",
    "\n",
    "from novel_sampling import over_sample, BiasedBinarySampling\n",
    "from generic_optimizer import GenericOptimizer, prepare_splits\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RjKim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\RjKim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 556, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1038, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "baselines = {}\n",
    "for folder in os.listdir('../Datasets'):\n",
    "    \n",
    "    x, y, splits = prepare_splits(os.path.join('../Datasets', folder, f\"{folder}.csv\"))\n",
    "    iteration_values = []\n",
    "    for train_idx, test_idx, validation_idx in splits: # 31 splits for Wilcoxon rank-sum\n",
    "        x_train, y_train = x[train_idx], y[train_idx]\n",
    "        x_test, y_test = x[test_idx], y[test_idx]\n",
    "\n",
    "        baseline_error = GenericOptimizer.calculate_overall_error(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            x_test,\n",
    "            y_test,\n",
    "            GenericOptimizer.n_neighbours\n",
    "        )\n",
    "\n",
    "        iteration_values.append(baseline_error)\n",
    "        \n",
    "    baselines[folder] = iteration_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(execution_config):\n",
    "    x_train = execution_config[\"x\"][execution_config[\"train_idx\"]]\n",
    "    y_train = execution_config[\"y\"][execution_config[\"train_idx\"]]\n",
    "    \n",
    "    if execution_config[\"Over Sample\"] == \"over_sample\": \n",
    "        x_train, y_train = over_sample(x_train, y_train)\n",
    "    \n",
    "    x_validation = execution_config[\"x\"][execution_config[\"validation_idx\"]]\n",
    "    y_validation = execution_config[\"y\"][execution_config[\"validation_idx\"]]\n",
    "    x_test = execution_config[\"x\"][execution_config[\"test_idx\"]]\n",
    "    y_test = execution_config[\"y\"][execution_config[\"test_idx\"]]\n",
    "    \n",
    "    validation_metrics = execution_config[\"Result\"].F\n",
    "    optimization_population = execution_config[\"Result\"].X\n",
    "    validation_pareto_front, best_x_train, best_y_train = GenericOptimizer.calculate_optimal_instance(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_validation,\n",
    "        y_validation,\n",
    "        validation_metrics,\n",
    "        optimization_population,\n",
    "        GenericOptimizer.n_neighbours\n",
    "    )\n",
    "    optimized_test_err = GenericOptimizer.calculate_overall_error(\n",
    "        best_x_train,\n",
    "        best_y_train,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        GenericOptimizer.n_neighbours\n",
    "    )\n",
    "    optimized_validation_err = GenericOptimizer.calculate_overall_error(\n",
    "        best_x_train,\n",
    "        best_y_train,\n",
    "        x_validation,\n",
    "        y_validation,\n",
    "        GenericOptimizer.n_neighbours\n",
    "    )\n",
    "    \n",
    "    if False:\n",
    "        test_metrics = []\n",
    "        for objective in experiment_config['objectives_list']:\n",
    "            test_metrics.append(GenericOptimizer.unbound_eval_objectives(\n",
    "                objective,\n",
    "                optimization_population[validation_pareto_front],\n",
    "                x_train, y_train,\n",
    "                x_test, y_test\n",
    "            ))\n",
    "        test_metrics = np.column_stack(test_metrics)\n",
    "        test_pareto_front, best_x_TEST, best_y_TEST = GenericOptimizer.calculate_optimal_instance(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            x_test,\n",
    "            y_test,\n",
    "            test_metrics,\n",
    "            optimization_population[validation_pareto_front],\n",
    "            GenericOptimizer.n_neighbours\n",
    "        )\n",
    "        rank_one_pareto_front_metrics = test_metrics[test_pareto_front]\n",
    "        x = rank_one_pareto_front_metrics[:,0]\n",
    "        y = rank_one_pareto_front_metrics[:,1]\n",
    "        x1 = validation_metrics[validation_pareto_front][:,0]\n",
    "        y1 = validation_metrics[validation_pareto_front][:,1]\n",
    "        plt.scatter(x, y)\n",
    "        plt.scatter(x1, y1)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.show()\n",
    "        print(metrics)\n",
    "        print(fronts)\n",
    "\n",
    "    return execution_config, optimized_test_err\n",
    "\n",
    "execution_configs = []\n",
    "path_string = \"optimizations\"\n",
    "for file in os.listdir(path_string):\n",
    "    with open(os.path.join(path_string, file), 'rb') as fh:\n",
    "        execution_configs.append(pickle.load(fh))\n",
    "        \n",
    "output = Parallel(n_jobs=-1)(delayed(calculate)(execution_config) for execution_config in execution_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = {}\n",
    "for config, optimized_error in output:\n",
    "    execution_type = config[\"Dataset\"] + \"<>\" + \"_\".join(config[\"Run Name\"].split(\"_\")[1:])\n",
    "\n",
    "    if execution_type not in aggregated_results:\n",
    "        aggregated_results[execution_type] = []\n",
    "\n",
    "    aggregated_results[execution_type].append(optimized_error) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over_sample_rand_population_australian_calculate_overall_error_calculate_num_examples\n",
      "mean: 0.3596867424948722\n",
      "median: 0.35838150289017345\n",
      "std_dev: 0.03096180976645582\n",
      "min_value: 0.3005780346820809\n",
      "max_value: 0.4508670520231214\n",
      "variance: 0.000958633664014199\n",
      "P-val: 0.04042779918502612\n",
      "--------------------------------\n",
      "\n",
      "over_sample_rand_population_bupa_calculate_overall_error_calculate_num_examples\n",
      "mean: 0.36818687430478314\n",
      "median: 0.367816091954023\n",
      "std_dev: 0.05418752867426563\n",
      "min_value: 0.27586206896551724\n",
      "max_value: 0.48275862068965514\n",
      "variance: 0.0029362882638243595\n",
      "P-val: 0.060733770489941684\n",
      "--------------------------------\n",
      "\n",
      "over_sample_rand_population_glass1_calculate_overall_error_calculate_num_examples\n",
      "mean: 0.25985663082437277\n",
      "median: 0.2592592592592593\n",
      "std_dev: 0.07606114237704661\n",
      "min_value: 0.12962962962962965\n",
      "max_value: 0.42592592592592593\n",
      "variance: 0.0057852973797013554\n",
      "P-val: 0.001823251474332668\n",
      "--------------------------------\n",
      "\n",
      "over_sample_rand_population_magic_calculate_overall_error_calculate_num_examples\n",
      "mean: 0.20303439987982572\n",
      "median: 0.20420609884332286\n",
      "std_dev: 0.002423679619059491\n",
      "min_value: 0.19894847528916926\n",
      "max_value: 0.20609884332281814\n",
      "variance: 5.87422289584436e-06\n",
      "P-val: 0.0025132156779548082\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in aggregated_results:\n",
    "    dataset = key.split(\"<>\")[0]\n",
    "    optimized_distribution = aggregated_results[key]\n",
    "    baseline_distribution = baselines[dataset]\n",
    "\n",
    "    wilcoxon_test = ranksums(baseline_distribution, optimized_distribution, alternative='less')\n",
    "\n",
    "    mean = np.mean(optimized_distribution)\n",
    "    median = np.median(optimized_distribution)\n",
    "    std_dev = np.std(optimized_distribution)\n",
    "    min_value = np.min(optimized_distribution)\n",
    "    max_value = np.max(optimized_distribution)\n",
    "    variance = np.var(optimized_distribution)\n",
    "\n",
    "    print(key.split(\"<>\")[1])\n",
    "    print(f\"mean: {mean}\")\n",
    "    print(f\"median: {median}\")\n",
    "    print(f\"std_dev: {std_dev}\")\n",
    "    print(f\"min_value: {min_value}\")\n",
    "    print(f\"max_value: {max_value}\")\n",
    "    print(f\"variance: {variance}\")\n",
    "    print(f\"P-val: {wilcoxon_test.pvalue}\")\n",
    "\n",
    "    print(\"--------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m execution_types = {}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result_name, test_err \u001b[38;5;129;01min\u001b[39;00m \u001b[43moutput\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m execution_types:\n\u001b[32m      4\u001b[39m         execution_types[result_name] = []\n",
      "\u001b[31mNameError\u001b[39m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "execution_types = {}\n",
    "for result_name, test_err in output:\n",
    "    if result_name not in execution_types:\n",
    "        execution_types[result_name] = []\n",
    "    execution_types[result_name].append(test_err)\n",
    "\n",
    "len(execution_types[\"over_sample_rand_population_australian_calculate_overall_error_calculate_num_examples\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
