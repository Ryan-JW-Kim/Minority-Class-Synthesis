{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a124fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "from pymoo.operators.mutation.bitflip import BitflipMutation, Mutation\n",
    "from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
    "from pymoo.operators.sampling.rnd import BinaryRandomSampling, Sampling\n",
    "from pymoo.operators.crossover.hux import HUX\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.indicators.hv import Hypervolume\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from main import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('../data.pickle', 'rb') as fh:\n",
    "\tdata_mapper = pickle.load(fh)\n",
    "\n",
    "data_keys = list(data_mapper.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e0f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_name = {}\n",
    "synthetic_by_name = {}\n",
    "for file in os.listdir('results'):\n",
    "\tif 'csv' in file:\n",
    "\t\tsynthetic_by_name[file] = pd.read_csv(f\"results/{file}\")\n",
    "\t\t\n",
    "\telse:\n",
    "\t\twith open(f\"results/{file}\", 'rb') as fh:\n",
    "\t\t\tresults_by_name[file] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997396f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 'abalone-20_vs_8-9-10'\n",
      "31 'abalone19'\n",
      "30 'abalone9-18'\n",
      "31 'cleveland-0_vs_4'\n",
      "31 'ecoli-0-1-4-7_vs_2-3-5-6'\n",
      "31 'ecoli-0-2-6-7_vs_3-5'\n",
      "31 'ecoli-0-4-6_vs_5'\n",
      "31 'ecoli-0-6-7_vs_3-5'\n",
      "31 'ecoli1'\n",
      "31 'ecoli4'\n",
      "31 'glass-0-6_vs_5'\n",
      "31 'glass1'\n",
      "31 'glass6'\n",
      "31 'haberman'\n",
      "31 'pima'\n",
      "31 'winequality-red-8_vs_6-7'\n",
      "31 'wisconsin'\n",
      "31 'yeast-2_vs_4'\n",
      "30 'yeast4'\n"
     ]
    }
   ],
   "source": [
    "iter_by_datset_name = {}\n",
    "for file in results_by_name:\n",
    "\titer_name = file.replace(\".result\", '')\n",
    "\titer_num = iter_name.split(\"_\")[0]\n",
    "\tdataset_name = \"_\".join(iter_name.split(\"_\")[1:])\n",
    "\t\n",
    "\tif dataset_name not in iter_by_datset_name:\n",
    "\t\titer_by_datset_name[dataset_name] = []\n",
    "\t\n",
    "\titer_by_datset_name[dataset_name].append(file)\n",
    "\n",
    "for k in iter_by_datset_name:\n",
    "\tprint(len(iter_by_datset_name[k]), repr(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b36610",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = \"ecoli4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992d211",
   "metadata": {},
   "source": [
    "# Oversample versus no oversample (no optimization both schemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7ded33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone-20_vs_8-9-10 IR: 72.6923\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "abalone19 IR: 129.4375\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "abalone9-18 IR: 16.381\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "cleveland-0_vs_4 IR: 13.3333\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "ecoli-0-1-4-7_vs_2-3-5-6 IR: 10.2\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "ecoli-0-2-6-7_vs_3-5 IR: 9.1818\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "ecoli-0-4-6_vs_5 IR: 9.1\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "ecoli-0-6-7_vs_3-5 IR: 9.0909\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "ecoli1 IR: 3.4211\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "ecoli4 IR: 15.8\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "glass-0-6_vs_5 IR: 12.5\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "glass1 IR: 1.8158\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "glass6 IR: 6.6429\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "haberman IR: 2.825\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "pima IR: 1.8657\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "winequality-red-8_vs_6-7 IR: 46.4444\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "wisconsin IR: 1.8655\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "yeast-2_vs_4 IR: 9.28\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n",
      "yeast4 IR: 28.68\n",
      "Mean validation acc diff 0.0\n",
      "Mean test acc diff       0.0\n",
      "Mean validation auc diff 0.0\n",
      "Mean test auc diff       0.0\n",
      "\n",
      "Validation acc diff pval False\n",
      "Test acc diff pval       False\n",
      "\n",
      "Validation auc diff pval False\n",
      "Test auc diff pval       False\n",
      "\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for curr in iter_by_datset_name:\n",
    "\tvalidation_baseline = []\n",
    "\ttest_baseline = []\n",
    "\n",
    "\toversample_validation = []\n",
    "\toversample_test = []\n",
    "\n",
    "\trecord = []\n",
    "\tlists = {\n",
    "\t\t\"Validation baseline acc\": [],\n",
    "\t\t\"Test baseline acc\": [],\n",
    "\n",
    "\t\t\"Validation baseline auc\": [],\n",
    "\t\t\"Test baseline auc\": [],\n",
    "\n",
    "\t\t\"Validation oversample auc\": [],\n",
    "\t\t\"Test oversample auc\": [],\n",
    "\n",
    "\t\t\"Validation oversample acc\": [],\n",
    "\t\t\"Test oversample acc\": [],\n",
    "\n",
    "\t}\n",
    "\tfor file in iter_by_datset_name[curr]:\n",
    "\n",
    "\t\tresult = results_by_name[file]\n",
    "\t\tsynthetic_samples = synthetic_by_name[file.replace(\".result\", \".csv\")]\n",
    "\t\t\n",
    "\t\tdata_split = data_mapper[file.replace(\".result\", \"\")]\n",
    "\t\tx_train, y_train = data_split['x_train'], data_split['y_train']\n",
    "\t\tx_validation, y_validation = data_split['x_validation'], data_split['y_validation']\n",
    "\t\tx_test, y_test = data_split['x_test'], data_split['y_test']\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tminority_label = pd.DataFrame(y_train).value_counts().argmin()\n",
    "\t\t# x_SYNTH, y_SYNTH = np.concatenate((x_train, synthetic_samples)), np.concatenate((y_train, [minority_label] * len(synthetic_samples)))\n",
    "\n",
    "\t\t# Fit with baseline train\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=AUC_Optimizer.n_neighbours)\n",
    "\t\tmodel.fit(x_train, y_train)\n",
    "\n",
    "\t\ty_pred = model.predict(x_validation)\n",
    "\t\tbaseline_validation_acc = accuracy_score(y_validation, y_pred)\n",
    "\t\tbaseline_validation_auc = roc_auc_score(y_validation, y_pred)\n",
    "\t\t\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\tbaseline_test_acc = accuracy_score(y_test, y_pred)\n",
    "\t\tbaseline_test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\t\t# Fit with oversampled train\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=AUC_Optimizer.n_neighbours)\n",
    "\t\tmodel.fit(x_train, y_train)\n",
    "\t\t#  model.fit(x_SYNTH, y_SYNTH)\n",
    "\n",
    "\t\ty_pred = model.predict(x_validation)\n",
    "\t\toversample_validation_acc = accuracy_score(y_validation, y_pred)\n",
    "\t\toversample_validation_auc = roc_auc_score(y_validation, y_pred)\n",
    "\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\toversample_test_acc = accuracy_score(y_test, y_pred)\n",
    "\t\toversample_test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\t\tlists[\"Validation baseline acc\"].append(baseline_validation_acc)\n",
    "\t\tlists[\"Test baseline acc\"].append(baseline_test_acc)\n",
    "\n",
    "\t\tlists[\"Validation baseline auc\"].append(baseline_validation_auc)\n",
    "\t\tlists[\"Test baseline auc\"].append(baseline_test_auc)\n",
    "\n",
    "\t\tlists[\"Validation oversample auc\"].append(oversample_validation_auc)\n",
    "\t\tlists[\"Test oversample auc\"].append(oversample_test_auc)\n",
    "\n",
    "\t\tlists[\"Validation oversample acc\"].append(oversample_validation_acc)\n",
    "\t\tlists[\"Test oversample acc\"].append(oversample_test_acc)\n",
    "\n",
    "\tcounts = pd.DataFrame(y_train).value_counts()\n",
    "\tprint(curr, f\"IR: {round(counts.max()/counts.min(), 4)}\")\n",
    "\n",
    "\tprint(f\"Mean validation acc diff {np.mean(np.subtract(lists['Validation oversample acc'], lists['Validation baseline acc']))}\")\n",
    "\tprint(f\"Mean test acc diff       {np.mean(np.subtract(lists['Test oversample acc'], lists['Test baseline acc']))}\")\n",
    "\n",
    "\tprint(f\"Mean validation auc diff {np.mean(np.subtract(lists['Validation oversample auc'], lists['Validation baseline auc']))}\")\n",
    "\tprint(f\"Mean test auc diff       {np.mean(np.subtract(lists['Test oversample auc'], lists['Test baseline auc']))}\")\n",
    "\t\n",
    "\tprint(f\"\\nValidation acc diff pval {True if ranksums(lists['Validation baseline acc'], lists['Validation oversample acc']).pvalue < 0.05 else False}\")\n",
    "\tprint(f\"Test acc diff pval       {True if ranksums(lists['Test baseline acc'], lists['Test oversample acc']).pvalue < 0.05 else False}\")\n",
    "\n",
    "\tprint(f\"\\nValidation auc diff pval {True if ranksums(lists['Validation baseline auc'], lists['Validation oversample auc']).pvalue < 0.05 else False}\")\n",
    "\tprint(f\"Test auc diff pval       {True if ranksums(lists['Test baseline auc'], lists['Test oversample auc']).pvalue < 0.05 else False}\")\n",
    "\tprint(\"\\n--\\n\")\n",
    "\t\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760f02d",
   "metadata": {},
   "source": [
    "# Oversample + optimization versus baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ba9568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.config import Config\n",
      "Config.warnings['not_compiled'] = False\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     52\u001b[39m problem = AUC_Optimizer(\n\u001b[32m     53\u001b[39m \tx_SYNTH,\n\u001b[32m     54\u001b[39m \ty_SYNTH,\n\u001b[32m     55\u001b[39m \tx_validation,\n\u001b[32m     56\u001b[39m \ty_validation,\n\u001b[32m     57\u001b[39m )\n\u001b[32m     58\u001b[39m algorithm = NSGA2(\n\u001b[32m     59\u001b[39m \tpop_size=AUC_Optimizer.population_size, \n\u001b[32m     60\u001b[39m \tsampling=DiverseCustomSampling(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m \teliminate_duplicates=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     64\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m result = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m\t\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_gen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAUC_Optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <--- maybe increase\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m\t\u001b[49m\u001b[43msave_history\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     70\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m validation_aucs = []\n\u001b[32m     73\u001b[39m test_aucs = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\optimize.py:67\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m     algorithm.setup(problem, **kwargs)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# actually execute the algorithm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m res = \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[32m     70\u001b[39m res.algorithm = algorithm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\algorithm.py:138\u001b[39m, in \u001b[36mAlgorithm.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_next():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\algorithm.py:158\u001b[39m, in \u001b[36mAlgorithm.next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# call the advance with them after evaluation\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infills \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfills\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance(infills=infills)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# if the algorithm does not follow the infill-advance scheme just call advance\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\evaluator.py:69\u001b[39m, in \u001b[36mEvaluator.eval\u001b[39m\u001b[34m(self, problem, pop, skip_already_evaluated, evaluate_values_of, count_evals, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# evaluate the solutions (if there are any)\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(I) > \u001b[32m0\u001b[39m:\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# do the actual evaluation - call the sub-function to set the corresponding values to the population\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# update the function evaluation counter\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_evals:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\evaluator.py:90\u001b[39m, in \u001b[36mEvaluator._eval\u001b[39m\u001b[34m(self, problem, pop, evaluate_values_of, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m X = pop.get(\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# call the problem to evaluate the solutions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m out = \u001b[43mproblem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# for each of the attributes set it to the problem\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m out.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\problem.py:257\u001b[39m, in \u001b[36mProblem.evaluate\u001b[39m\u001b[34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     only_single_value = \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np.ndarray))\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m _out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m out = {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _out.items():\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\problem.py:299\u001b[39m, in \u001b[36mProblem.do\u001b[39m\u001b[34m(self, X, return_values_of, *args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m._evaluate_elementwise(X, out, *args, **kwargs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# finally format the output dictionary\u001b[39;00m\n\u001b[32m    302\u001b[39m out = \u001b[38;5;28mself\u001b[39m._format_dict(out, \u001b[38;5;28mlen\u001b[39m(X), return_values_of)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymoo\\core\\problem.py:307\u001b[39m, in \u001b[36mProblem._evaluate_vectorized\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate_vectorized\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, out, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MY FILES\\Dev\\CP493-Research-Project\\cVAE\\main.py:80\u001b[39m, in \u001b[36mAUC_Optimizer._evaluate\u001b[39m\u001b[34m(self, x, out, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m \tmodel.fit(\n\u001b[32m     75\u001b[39m \t\t\u001b[38;5;28mself\u001b[39m.X_train[instance], \n\u001b[32m     76\u001b[39m \t\t\u001b[38;5;28mself\u001b[39m.y_train[instance]\n\u001b[32m     77\u001b[39m \t)\n\u001b[32m     79\u001b[39m \ty_pred = model.predict(\u001b[38;5;28mself\u001b[39m.X_val)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \tinverse_AUC = \u001b[32m1\u001b[39m - \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m num_samples.append(np.sum(instance))\n\u001b[32m     83\u001b[39m values.append(inverse_AUC)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_ranking.py:640\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    639\u001b[39m     labels = np.unique(y_true)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     y_true = \u001b[43mlabel_binarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m0\u001b[39m]\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[32m    642\u001b[39m         partial(_binary_roc_auc_score, max_fpr=max_fpr),\n\u001b[32m    643\u001b[39m         y_true,\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         sample_weight=sample_weight,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\preprocessing\\_label.py:585\u001b[39m, in \u001b[36mlabel_binarize\u001b[39m\u001b[34m(y, classes, neg_label, pos_label, sparse_output)\u001b[39m\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m target data is not supported with label binarization\u001b[39m\u001b[33m\"\u001b[39m % y_type\n\u001b[32m    582\u001b[39m     )\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sparse_output:\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     Y = \u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     Y = Y.astype(\u001b[38;5;28mint\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    588\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neg_label != \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\scipy\\sparse\\_compressed.py:1181\u001b[39m, in \u001b[36m_cs_matrix.toarray\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1179\u001b[39m     y = out.T\n\u001b[32m   1180\u001b[39m M, N = x._swap(x._shape_as_2d)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43mcsr_todense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for curr in iter_by_datset_name:\n",
    "\tvalidation_baseline = []\n",
    "\ttest_baseline = []\n",
    "\n",
    "\toversample_validation = []\n",
    "\toversample_test = []\n",
    "\n",
    "\trecord = []\n",
    "\tlists = {\n",
    "\t\t\"Validation baseline auc\": [],\n",
    "\t\t\"Test baseline auc\": [],\n",
    "\n",
    "\t\t\"Validation baseline acc\": [],\n",
    "\t\t\"Test baseline acc\": [],\n",
    "\n",
    "\t\t\"Optimized Validation auc\": [],\n",
    "\t\t\"Optimized Test auc\": [],\n",
    "\t\t\"Ideal Test auc\": [],\n",
    "\n",
    "\t\t\"Optimized Validation acc\": [],\n",
    "\t\t\"Optimized Test acc\": [],\n",
    "\t\t\"Ideal Test acc\": [],\n",
    "\n",
    "\t}\n",
    "\tfor file in iter_by_datset_name[curr]:\n",
    "\n",
    "\t\tresult = results_by_name[file]\n",
    "\t\tsynthetic_samples = synthetic_by_name[file.replace(\".result\", \".csv\")]\n",
    "\t\t\n",
    "\t\tdata_split = data_mapper[file.replace(\".result\", \"\")]\n",
    "\t\tx_train, y_train = data_split['x_train'], data_split['y_train']\n",
    "\t\tx_validation, y_validation = data_split['x_validation'], data_split['y_validation']\n",
    "\t\tx_test, y_test = data_split['x_test'], data_split['y_test']\n",
    "\t\t\n",
    "\t\tminority_label = pd.DataFrame(y_train).value_counts().argmin()\n",
    "\n",
    "\t\t# Fit with baseline train\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=AUC_Optimizer.n_neighbours)\n",
    "\t\tmodel.fit(x_train, y_train)\n",
    "\n",
    "\t\ty_pred = model.predict(x_validation)\n",
    "\t\tbaseline_validation_acc = accuracy_score(y_validation, y_pred)\n",
    "\t\tbaseline_validation_auc = roc_auc_score(y_validation, y_pred)\n",
    "\t\t\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\tbaseline_test_acc = accuracy_score(y_test, y_pred)\n",
    "\t\tbaseline_test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\t\tx_SYNTH, y_SYNTH = np.concatenate((x_train, synthetic_samples)), np.concatenate((y_train, [minority_label] * len(synthetic_samples)))\n",
    "\n",
    "\t\t# Select ideal instance\n",
    "\t\tproblem = AUC_Optimizer(\n",
    "\t\t\tx_SYNTH,\n",
    "\t\t\ty_SYNTH,\n",
    "\t\t\tx_validation,\n",
    "\t\t\ty_validation,\n",
    "\t\t)\n",
    "\t\talgorithm = NSGA2(\n",
    "\t\t\tpop_size=AUC_Optimizer.population_size, \n",
    "\t\t\tsampling=DiverseCustomSampling(),\n",
    "\t\t\tcrossover=HUX(), \n",
    "\t\t\tmutation=BitflipMutation(), \n",
    "\t\t\teliminate_duplicates=True,\n",
    "\t\t)\n",
    "\t\tresult = minimize(\n",
    "\t\t\tproblem, \n",
    "\t\t\talgorithm, \n",
    "\t\t\t('n_gen', AUC_Optimizer.population_size), # <--- maybe increase\n",
    "\t\t\tsave_history=False\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tvalidation_aucs = []\n",
    "\t\ttest_aucs = []\n",
    "\t\t\n",
    "\t\tfor instance in result.X:\n",
    "\t\t\tmodel = KNeighborsClassifier(n_neighbors=AUC_Optimizer.n_neighbours)\n",
    "\t\t\tmodel.fit(x_SYNTH[instance], y_SYNTH[instance])\n",
    "\t\t\ty_pred = model.predict(x_validation)\n",
    "\t\t\tvalidation_aucs.append(roc_auc_score(y_validation, y_pred))\n",
    "\t\t\ty_pred = model.predict(x_test)\n",
    "\t\t\ttest_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "\t\tvalidation_idx = np.argmin(validation_aucs)\n",
    "\t\ttest_idx = np.argmin(test_aucs)\n",
    "\n",
    "\t\t# Calculate metrics using ideal instance w.r.t validation AUC\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=AUC_Optimizer.n_neighbours)\n",
    "\t\tmodel.fit(x_SYNTH[result.X[validation_idx]], y_SYNTH[result.X[validation_idx]])\n",
    "\n",
    "\t\ty_pred = model.predict(x_validation)\n",
    "\t\toptimized_validation_acc = accuracy_score(y_validation, y_pred)\n",
    "\t\toptimized_validation_auc = roc_auc_score(y_validation, y_pred)\n",
    "\t\t\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\toptimized_test_acc = accuracy_score(y_test, y_pred)\n",
    "\t\toptimized_test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\t\t# Calculate metrics using ideal instance w.r.t test AUC\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=AUC_Optimizer.n_neighbours)\n",
    "\t\tmodel.fit(x_SYNTH[result.X[test_idx]], y_SYNTH[result.X[test_idx]])\n",
    "\t\t\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\tideal_test_acc = accuracy_score(y_test, y_pred)\n",
    "\t\tideal_test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\t\tlists[\"Validation baseline acc\"].append(baseline_validation_acc)\n",
    "\t\tlists[\"Test baseline acc\"].append(baseline_test_acc)\n",
    "\n",
    "\t\tlists[\"Validation baseline auc\"].append(baseline_validation_auc)\n",
    "\t\tlists[\"Test baseline auc\"].append(baseline_test_auc)\n",
    "\n",
    "\t\tlists[\"Optimized Validation auc\"].append(optimized_validation_auc)\n",
    "\t\tlists[\"Optimized Test auc\"].append(optimized_test_auc)\n",
    "\t\tlists[\"Ideal Test auc\"].append(ideal_test_auc)\n",
    "\n",
    "\t\tlists[\"Optimized Validation acc\"].append(optimized_validation_acc)\n",
    "\t\tlists[\"Optimized Test acc\"].append(optimized_test_acc)\n",
    "\t\tlists[\"Ideal Test acc\"].append(ideal_test_acc)\n",
    "\n",
    "\tcounts = pd.DataFrame(y_train).value_counts()\n",
    "\tprint(curr, f\"IR: {round(counts.max()/counts.min(), 4)}\")\n",
    "\n",
    "\tprint(f\"Mean optimized validation acc diff {np.mean(np.subtract(lists['Optimized Validation acc'], lists['Validation baseline acc']))}\")\n",
    "\tprint(f\"Mean optimized test acc diff       {np.mean(np.subtract(lists['Optimized Test acc'], lists['Test baseline acc']))}\")\n",
    "\tprint(f\"Mean ideal test acc diff           {np.mean(np.subtract(lists['Ideal Test acc'], lists['Test baseline acc']))}\")\n",
    "\t\n",
    "\tprint(f\"Mean optimized validation auc diff {np.mean(np.subtract(lists['Optimized Validation auc'], lists['Validation baseline auc']))}\")\n",
    "\tprint(f\"Mean optimized test auc diff       {np.mean(np.subtract(lists['Optimized Test auc'], lists['Test baseline auc']))}\")\n",
    "\tprint(f\"Mean ideal test auc diff           {np.mean(np.subtract(lists['Ideal Test auc'], lists['Test baseline auc']))}\")\n",
    "\t\n",
    "\tprint(f\"\\nValidation acc diff pval         {True if ranksums(lists['Validation baseline acc'], lists['Optimized Validation acc']).pvalue < 0.05 else False}\")\n",
    "\tprint(f\"Test acc diff pval                 {True if ranksums(lists['Test baseline acc'], lists['Optimized Test acc']).pvalue < 0.05 else False}\")\n",
    "\tprint(f\"Ideal Test acc diff pval           {True if ranksums(lists['Test baseline acc'], lists['Ideal Test acc']).pvalue < 0.05 else False}\")\n",
    "\n",
    "\tprint(f\"\\nValidation auc diff pval         {True if ranksums(lists['Validation baseline auc'], lists['Optimized Validation auc']).pvalue < 0.05 else False}\")\n",
    "\tprint(f\"Test auc diff pval                 {True if ranksums(lists['Test baseline auc'], lists['Test baseline auc']).pvalue < 0.05 else False}\")\n",
    "\tprint(f\"Ideal Test auc diff pval           {True if ranksums(lists['Test baseline auc'], lists['Ideal Test auc']).pvalue < 0.05 else False}\")\n",
    "\tprint(\"\\n--\\n\")\n",
    "\t\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((958, 7), (970, 7), (970,), (6, 981))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.concatenate((x_train, synthetic_samples))\n",
    "temp1 = np.concatenate((y_train, [minority_label] * len(synthetic_samples)))\n",
    "\n",
    "x_train.shape, temp.shape, temp1.shape, result.X.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
